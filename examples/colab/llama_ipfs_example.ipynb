{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-ipfs Example in Google Colab\n",
    "\n",
    "This notebook demonstrates how to use llama-cpp-python with IPFS models using the llama-ipfs integration.\n",
    "\n",
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama-cpp-python, huggingface-hub, and llama-ipfs\n",
    "!pip install llama-cpp-python huggingface-hub llama-ipfs --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Activate IPFS Integration\n",
    "\n",
    "In most environments, you would use `llama-ipfs activate`, but in Google Colab we need to apply the patch explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard activation approach (commented out)\n",
    "# llama-ipfs activate\n",
    "\n",
    "# For Google Colab, we need to apply the patch explicitly:\n",
    "import llama_ipfs\n",
    "\n",
    "# Apply the patch manually\n",
    "llama_ipfs.activate()\n",
    "\n",
    "# Verify that the patch was applied successfully\n",
    "print(f\"IPFS patch active: {llama_ipfs.status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Use an IPFS-hosted Model\n",
    "\n",
    "Now we can load llama-cpp models directly from IPFS using their CID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load model directly from IPFS\n",
    "repo_id = \"ipfs://bafybeie7quk74kmqg34nl2ewdwmsrlvvt6heayien364gtu2x6g2qpznhq\"\n",
    "# Equivalent HuggingFace model: repo_id = \"aisuko/gpt2-117M-gguf\"\n",
    "filename = \"ggml-model-Q4_K_M.gguf\"\n",
    "\n",
    "# Load the model with minimal verbosity\n",
    "llm = Llama.from_pretrained(repo_id=repo_id, filename=filename, verbose=False)\n",
    "\n",
    "# Prepare a simple Q&A example\n",
    "context = (\n",
    "    \"France is a country in Western Europe known for its rich history and culture. \"\n",
    "    \"It is home to many famous landmarks including the Eiffel Tower. Its capital is Paris.\"\n",
    ")\n",
    "question = \"What is the capital of France?\"\n",
    "prompt = f\"Context: {context}\\nQuestion: {question}\\nBased solely on the above context, answer the question in one word:\"\n",
    "\n",
    "# Generate the answer\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=10,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.0,\n",
    "    stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "# Extract and print the answer\n",
    "answer = output['choices'][0]['text'].strip()\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "The `llama-ipfs` package patches the llama-cpp-python library to:\n",
    "\n",
    "1. Recognize `ipfs://` URIs as valid model identifiers\n",
    "2. Download model files from IPFS nodes or gateways\n",
    "3. Cache models locally for faster loading in subsequent runs\n",
    "\n",
    "This allows you to load models from a decentralized network without changing any of your existing code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
